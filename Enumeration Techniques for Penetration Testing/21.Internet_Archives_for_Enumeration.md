# Subdomain Enumeration Guide

## Internet Archives for Enumeration

Internet Archives (like the Wayback Machine) are large-scale crawlers that store historical snapshots of websites. They are valuable for subdomain enumeration because they often reveal infrastructure that no longer appears in current DNS records [file:35].

### Benefits
*   Discover **old, forgotten, or deprecated subdomains**.
*   Generate **permutations** to find new valid subdomains.
*   Improve **passive reconnaissance**.

### 1. Fetching Historical URLs
Tools like `waybackurls` and `gau` are used to retrieve these archived paths.

**Using waybackurls**
Fetches all known URLs from the Wayback Machine.

    waybackurls tesla.com > tesla.com-waybackurls.txt

### Using gau (GetAllUrls)
Pulls URLs from multiple sources (Wayback Machine, Common Crawl, URLScan, VirusTotal).


    gau tesla.com > tesla.com-gau.txt

### 2. Extracting Unique Subdomains
Archive tools return full URLs, but often only the unique subdomains are needed. The tool unfurl is used to extract domain names [file:36].

### Command:

    cat tesla.com-gau.txt | unfurl -u domains | sort -u > subdomains.txt

### Explanation:

- unfurl -u domains: Extracts the domain name from the URL path.

- sort -u: Removes duplicate entries.

### Generating Subdomain Permutations (dnsgen)

#### About dnsgen

dnsgen is a permutation-based subdomain generation tool widely used in reconnaissance, bug bounty hunting, and penetration testing. It creates possible subdomains by applying common patterns, prefixes, suffixes, and mutations to known domains or subdomains.

#### What dnsgen Does

- Takes: A base domain or list of known subdomains (and optional wordlists).

- Generates: New subdomain permutations and common development, staging, API, and infrastructure variants.

Note: Output from dnsgen is not validated. Always resolve results using a DNS resolver.

#### Installation

#### Using pip:

    pip install dnsgen

#### From source:

    git clone https://github.com/ProjectAnte/dnsgen.git
    cd dnsgen
    python3 setup.py install

#### Basic Usage

#### Generate subdomains from a domain:

    dnsgen tesla.com

#### Generate subdomains using a wordlist:

    dnsgen -w deepmagic.com-prefixes-top500.txt -f tesla.com

- w: Wordlist for prefixes/suffixes

- f: Target domain

### Typical Recon Pipeline
dnsgen is usually chained with a DNS resolution tool (like dnsx) to filter for live domains.

    dnsgen -w deepmagic.com-prefixes-top500.txt -f tesla.com | dnsx -silent

### Flow:

1. Generate permutations with dnsgen.

2. Resolve valid domains using dnsx.

3. Output only live subdomains.

Using Existing Subdomains as Input:

    cat subdomains.txt | dnsgen | dnsx -silent

### Used to:

- Expand real subdomain patterns

- Discover hidden environments

### Common Mutation Patterns

dnsgen generates combinations such as:

- dev.example.com

- api.example.com

- example-dev.com

- example-api.com

- dev-api.example.com

### Use Cases

- Bug bounty reconnaissance

- Asset discovery

- Finding staging / test environments

- Post Wayback or CRT.sh enumeration

### Best Practices

#### Always validate output with:

- dnsx

- massdns

- puredns

### Combine dnsgen with:

- subfinder

- amass

- Internet archive data

### Helpful Tools to Pair With dnsgen

    Tool	             Purpose
    subfinder	         Passive subdomain discovery
    amass              	Active & passive enumeration
    dnsx               	Fast DNS resolution
    httpx              	Live HTTP probing

### References

- GitHub: https://github.com/ProjectAnte/dnsgen

- Wordlists: https://github.com/danielmiessler/SecLists

## Validating and Resolving Subdomains

Generated subdomains must be validated to confirm they actually exist.

### DNS Resolution with dnsx

    cat permutations.txt | dnsx -resp > valid_subdomains.txt

- dnsx checks whether domains resolve

- -resp includes DNS response details for analysis

### Checking Live Web Services with httprobe

    cat valid_subdomains.txt | httprobe > live_subdomains.txt


- Identifies which subdomains expose HTTP or HTTPS services

- Helps prioritize targets for further testing

### Automating the Full Workflow

    echo "tesla.com" | waybackurls \
      | unfurl -u domains \
      | sort -u \
      | tee subdomains.txt
    
    cat subdomains.txt \
      | dnsgen - \
      | tee permutations.txt
    
    cat permutations.txt \
      | dnsx -resp \
      | tee valid_subdomains.txt
    
    cat valid_subdomains.txt \
      | httprobe \
      | tee live_subdomains.txt

### This pipeline covers:

1. Historical URL extraction

2. Subdomain extraction

3. Permutation generation

4. DNS validation

5. Live service discovery

### Why This Approach Works

Internet archives often contain infrastructure history that organizations forget to clean up.

Even if a subdomain is no longer publicly linked, it may:

- Still resolve in DNS

- Host legacy APIs

- Expose admin panels or staging environments

### Conclusion

By leveraging Internet Archives and tools like:

- waybackurls

- gau / gauplus

- unfurl

- dnsgen

- dnsx

- httprobe

You can perform deep, passive subdomain enumeration beyond traditional wordlists or search engines.

#### Especially useful for:

- ðŸ” Security researchers

- ðŸž Bug bounty hunters

- ðŸ›¡ï¸ Organizations auditing their attack surface

# Internet Archives â€“ URL & Subdomain Enumeration

A collection of **passive reconnaissance techniques** for extracting historical URLs and discovering subdomains using Internet Archiveâ€“based tools.

---

## gauplus

### GitHub Repository
https://github.com/bp0lr/gauplus

**gauplus** is an enhanced version of **gau (GetAllURLs)**.  
It aggregates archived URLs from multiple public sources, making it useful for:

- Reconnaissance
- Endpoint discovery
- Subdomain enumeration

---

### Data Sources Used by gauplus

`gauplus` collects URLs from several well-known archives:

- **Wayback Machine** â€“ Historical snapshots of websites
- **Common Crawl** â€“ Large-scale web crawl datasets
- **VirusTotal** â€“ URLs observed via malware and threat-intelligence feeds
- **URLScan.io** â€“ URLs captured during website scans
- **Other public archives** â€“ Additional passive data sources

---

### Installation

Ensure **Go** is installed and configured, then run:

    go install github.com/bp0lr/gauplus@latest

#### Verify installation:

    gauplus -h

#### Usage

Fetch URLs for a Domain
Retrieve archived URLs for a domain (including subdomains):

    gauplus -t 5 -random-agent -subs tesla.com

Save output to a file:

    gauplus -t 5 -random-agent -subs tesla.com > tesla.com-gauplus.txt

#### Explanation of Common Flags
- -t 5
Number of concurrent threads (default: 1). Higher values increase speed.

- -random-agent
Uses random User-Agent strings to reduce blocking or rate limiting.

- -subs
Includes all discovered subdomains in the results.

### Extracting Only Subdomains
Since gauplus outputs full URLs, use unfurl to extract domain names.

Extract domains to stdout:

    cat tesla.com-gauplus.txt | unfurl -u domains

Save extracted domains to a file:

    cat tesla.com-gauplus.txt | unfurl -u domains > tesla.com-domains.txt

One-liner (recommended):

    gauplus -t 5 -random-agent -subs tesla.com | unfurl -u domains > tesla.com-domains.txt

### Example Output

    www.tesla.com
    shop.tesla.com
    blog.tesla.com
    api.tesla.com
    energy.tesla.com

### Tips & Best Practices

- Remove duplicates:

      gauplus -subs tesla.com | unfurl -u domains | sort -u

- Combine with tools such as:

   - httpx
    
   -  nuclei
    
   -  ffuf

- Best suited for passive reconnaissance (no direct interaction with target)

#### Conclusion
gauplus is a powerful passive reconnaissance tool that aggregates URLs from multiple archival sources.
When combined with unfurl, it becomes an effective solution for:

- Subdomain enumeration

- Hidden endpoint discovery

- Bug bounty and penetration testing workflows

#### waybackurls

GitHub Repository

https://github.com/tomnomnom/waybackurls

waybackurls is a tool developed by tomnomnom that extracts URLs from the Wayback Machine (Internet Archive) for a given domain.

#### Useful for:

- Reconnaissance

- Subdomain enumeration

- Discovering old or forgotten endpoints

### Installation

    go install github.com/tomnomnom/waybackurls@latest

Ensure your Go environment is properly configured.

### Usage
Fetch URLs from Wayback Machine

    waybackurls tesla.com

Save output to a file:

    waybackurls tesla.com > tesla.com-waybackurls.txt

### Extracting Only Subdomains

    waybackurls tesla.com | unfurl -u domains

### Saving Output to a File

    waybackurls tesla.com | unfurl -u domains > waybackurls-output.txt

### Example Output

    www.tesla.com
    shop.tesla.com
    blog.tesla.com
    assets.tesla.com

### Conclusion
waybackurls is a powerful tool for gathering historical web data.
When combined with unfurl, it enables efficient subdomain extraction for further security analysis.
